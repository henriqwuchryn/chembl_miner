{
        'BaggingRegressor': {
            'n_estimators': Integer(lower=10, upper=1000), #10
            'max_samples': Continuous(lower=0.1, upper=1.0),#1.0
            'max_features': Continuous(lower=0.1, upper=1.0),#1.0
            'bootstrap': Categorical(choices=[True, False]),  # Whether samples are drawn with replacement
            'bootstrap_features': Categorical(choices=[True, False])  # Whether features are drawn with replacement
        },
        'GradientBoostingRegressor': {
            'n_estimators': Integer(lower=100, upper=2000), #100
            'learning_rate': Continuous(lower=0.001, upper=1), #0.1
            'max_depth': Integer(lower=3, upper=100), #3
            'min_samples_split': Integer(lower=2, upper=20), #2
            'min_samples_leaf': Integer(lower=1, upper=20), #1
            'subsample': Continuous(lower=0.1,upper=1.0),  # 1.0
            'max_features': Continuous(lower=0.1, upper=1.0)  # 1.0
        },
        'LGBMRegressor': {
            'n_estimators': [100, 200, 400], #100
            'learning_rate': [0.05, 0.1, 0.2],#0.1
            'max_depth': [6, 9, 18],#-1
            'num_leaves': [31, 62],#31
            'min_child_samples': [20, 40],#20
            'subsample': [0.7, 1.0],  # Fraction of samples used for fitting
            'colsample_bytree': [0.7, 1.0],  # Fraction of features used for fitting
            'reg_alpha': [0, 0.1, 1],  # L1 regularization
            'reg_lambda': [0, 0.1, 1],  # L2 regularization
            'force_row_wise': [True]
        },
        'RandomForestRegressor': {
            'n_estimators': Integer(lower=100, upper=2000), #100
            'max_depth': Categorical([None]), #None
            'min_samples_split': Integer(lower=2, upper=20), #2
            'min_samples_leaf': Integer(lower=1, upper=20), #1
            'max_features': Continuous(lower=0.1, upper=1),  # 1.0
            'bootstrap': Categorical(choices=[True, False]),  # Whether bootstrap samples are used
        },
        'XGBRegressor': {
            'n_estimators': Integer(lower=100, upper=2000),
            'learning_rate': Continuous(lower=0.001, upper=1),
            'max_depth': Integer(lower=0, upper=100),
            'min_child_weight': Continuous(lower=0.1, upper=2.0),  # Minimum sum of instance weight needed in a child
            'gamma': Continuous(lower=0.1, upper=2.0),  # Minimum loss reduction required to make a split
            'subsample': Continuous(lower=0.1, upper=1),  # Fraction of samples used for fitting
            'colsample_bytree': Continuous(lower=0.1, upper=1),  # Fraction of features used for fitting
            'reg_alpha': Continuous(lower=0.1, upper=2.0),  # L1 regularization
            'reg_lambda': Continuous(lower=0.1, upper=2.0)  # L2 regularization
        },
        'HistGradientBoostingRegressor': {
            'loss': Categorical(choices=['squared_error','absolute_error','gamma','quantile']), #squared_error
            'max_iter': Integer(lower=100, upper=2000), #100
            'learning_rate': Continuous(lower=0.1, upper=1), #0.1
            'max_depth': Categorical(choices=[None]), #None
            'min_samples_leaf': Integer(lower=10, upper=200), #20
            'max_leaf_nodes': Integer(lower=10, upper=200),  #61
            'l2_regularization': Continuous(lower=0.1, upper=2.0),  #0
            'max_bins': Integer(lower=100, upper=255)  #255
        },
        'ExtraTreesRegressor': {
            'n_estimators': Integer(lower=100, upper=2000), #100
            'max_depth': Categorical([None]), #None
            'min_samples_split': Integer(lower=2, upper=20), #2
            'min_samples_leaf': Integer(lower=1, upper=20), #1
            'max_features': Continuous(lower=0.1, upper=1),  # Number of features to consider for splits
            'bootstrap': Categorical(choices=[True, False]),  # Whether bootstrap samples are used
        }
    }
